using System.Net.WebSockets;
using System.Security.Cryptography;
using System.Text;
using System.Text.Json;
using System.Diagnostics;
using System.Net.Http.Json;
using Olbrasoft.Mediation;
using VoiceAssistant.Shared.Data.Queries.SpeechLockQueries;

namespace EdgeTtsWebSocketServer.Services;

public class EdgeTtsService
{
    private const string WSS_URL = "wss://api.msedgeservices.com/tts/cognitiveservices/websocket/v1";
    private const string API_KEY = "6A5AA1D4EAFF4E9FB37E23D68491D6F4";
    private const string CHROMIUM_FULL_VERSION = "140.0.3485.14";
    private const long WIN_EPOCH = 11644473600;
    private const double S_TO_NS = 1e9;
    
    private readonly string _cacheDirectory;
    private readonly string _speechLockFile;
    private readonly string _defaultVoice;
    private readonly string _defaultRate;
    private readonly string _listenerApiUrl;
    private readonly ILogger<EdgeTtsService> _logger;
    private readonly IServiceProvider _serviceProvider;
    private readonly AssistantSpeechStateService _assistantSpeechState;
    private readonly HttpClient _httpClient;
    
    // Current playback process for stop functionality
    private Process? _currentPlaybackProcess;
    private readonly object _processLock = new();

    public EdgeTtsService(
        IConfiguration configuration, 
        ILogger<EdgeTtsService> logger, 
        IServiceProvider serviceProvider,
        AssistantSpeechStateService assistantSpeechState,
        HttpClient httpClient)
    {
        _logger = logger;
        _serviceProvider = serviceProvider;
        _assistantSpeechState = assistantSpeechState;
        _httpClient = httpClient;
        _cacheDirectory = ExpandPath(configuration["EdgeTts:CacheDirectory"] ?? "~/.cache/edge-tts-server");
        _speechLockFile = configuration["EdgeTts:SpeechLockFile"] ?? "/tmp/speech.lock";
        _defaultVoice = configuration["EdgeTts:DefaultVoice"] ?? "cs-CZ-AntoninNeural";
        _defaultRate = configuration["EdgeTts:DefaultRate"] ?? "+20%";
        _listenerApiUrl = configuration["EdgeTts:ListenerApiUrl"] ?? "http://localhost:5051";
        
        Directory.CreateDirectory(_cacheDirectory);
    }

    /// <summary>
    /// Check if speech is locked (user is recording).
    /// </summary>
    private async Task<bool> IsSpeechLockedAsync()
    {
        try
        {
            using var scope = _serviceProvider.CreateScope();
            var mediator = scope.ServiceProvider.GetRequiredService<IMediator>();
            
            var query = new SpeechLockExistsQuery { MaxAgeMinutes = 1 };
            return await mediator.MediateAsync(query);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to check speech lock from DB, allowing speech");
            return false; // Fail open - allow speech if DB check fails
        }
    }

    /// <summary>
    /// Check if CapsLock LED is currently ON (user is recording via push-to-talk).
    /// This is a direct hardware check as a backup to database lock.
    /// </summary>
    private bool IsCapsLockOn()
    {
        try
        {
            // Check all potential CapsLock LED paths
            var ledPaths = new[]
            {
                "/sys/class/leds/input0::capslock/brightness",
                "/sys/class/leds/input1::capslock/brightness",
                "/sys/class/leds/input2::capslock/brightness",
                "/sys/class/leds/input3::capslock/brightness"
            };

            foreach (var path in ledPaths)
            {
                if (File.Exists(path))
                {
                    var value = File.ReadAllText(path).Trim();
                    if (value == "1")
                    {
                        _logger.LogDebug("CapsLock LED is ON (checked {Path})", path);
                        return true;
                    }
                }
            }

            // Also try dynamic discovery
            if (Directory.Exists("/sys/class/leds"))
            {
                foreach (var dir in Directory.GetDirectories("/sys/class/leds"))
                {
                    if (dir.Contains("capslock", StringComparison.OrdinalIgnoreCase))
                    {
                        var brightnessFile = Path.Combine(dir, "brightness");
                        if (File.Exists(brightnessFile))
                        {
                            var value = File.ReadAllText(brightnessFile).Trim();
                            if (value == "1")
                            {
                                _logger.LogDebug("CapsLock LED is ON (discovered {Path})", brightnessFile);
                                return true;
                            }
                        }
                    }
                }
            }

            return false;
        }
        catch (Exception ex)
        {
            _logger.LogDebug(ex, "Could not check CapsLock LED state");
            return false; // Fail open - allow speech if check fails
        }
    }

    /// <summary>
    /// Check if speech should be blocked (either by DB lock or CapsLock state).
    /// </summary>
    private async Task<bool> ShouldBlockSpeechAsync()
    {
        // Check 1: Database lock (set by PTT service)
        if (await IsSpeechLockedAsync())
        {
            _logger.LogDebug("Speech blocked: DB lock exists");
            return true;
        }

        // Check 2: Direct CapsLock LED state (backup check)
        if (IsCapsLockOn())
        {
            _logger.LogDebug("Speech blocked: CapsLock LED is ON");
            return true;
        }

        return false;
    }

    public async Task<(bool success, string message, bool cached)> SpeakAsync(
        string text, 
        string? voice = null, 
        string? rate = null,
        string? volume = null,
        string? pitch = null,
        bool play = true)
    {
        try
        {
            // Check speech lock from database or CapsLock - if locked, silently skip TTS
            if (await ShouldBlockSpeechAsync())
            {
                _logger.LogInformation("Speech blocked - skipping TTS for: {Text}", text);
                return (true, string.Empty, false);
            }

            voice ??= _defaultVoice;
            rate ??= _defaultRate;
            volume ??= "+0%";
            pitch ??= "+0Hz";

            // Generate cache file name
            var cacheFileName = GenerateCacheFileName(text, voice, rate, volume, pitch);
            var cacheFilePath = Path.Combine(_cacheDirectory, cacheFileName);

            // Check cache
            if (File.Exists(cacheFilePath))
            {
                _logger.LogInformation("Playing from cache: {Text}", text);
                if (play)
                {
                    await PlayAudioAsync(cacheFilePath, text);
                    return (true, $"✅ Played from cache: {text}", true);
                }
                return (true, $"✅ Audio cached at: {cacheFilePath}", true);
            }

            // Generate new audio via WebSocket
            var audioData = await GenerateAudioAsync(text, voice, rate, volume, pitch);
            
            if (audioData == null || audioData.Length == 0)
            {
                return (false, "❌ Failed to generate audio", false);
            }

            // Save to cache
            await File.WriteAllBytesAsync(cacheFilePath, audioData);
            _logger.LogInformation("Saved to cache: {CacheFile}", cacheFilePath);

            // Play audio
            if (play)
            {
                await PlayAudioAsync(cacheFilePath, text);
                return (true, $"✅ Generated and played: {text}", false);
            }

            return (true, $"✅ Audio generated at: {cacheFilePath}", false);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Error in SpeakAsync");
            return (false, $"❌ Error: {ex.Message}", false);
        }
    }

    private async Task<byte[]?> GenerateAudioAsync(string text, string voice, string rate, string volume, string pitch)
    {
        using var client = new ClientWebSocket();
        
        // Add all required headers to match Microsoft Edge TTS requirements
        client.Options.SetRequestHeader("User-Agent", 
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36 Edg/140.0.0.0");
        client.Options.SetRequestHeader("Origin", "chrome-extension://jdiccldimpdaibmpdkjnbmckianbfold");
        client.Options.SetRequestHeader("Pragma", "no-cache");
        client.Options.SetRequestHeader("Cache-Control", "no-cache");
        client.Options.SetRequestHeader("Accept-Encoding", "gzip, deflate, br");
        client.Options.SetRequestHeader("Accept-Language", "en-US,en;q=0.9");
        
        // CRITICAL: Add WebSocket subprotocol - this is required by Microsoft Edge TTS
        client.Options.AddSubProtocol("synthesize");
        
        // Generate connection parameters
        var connectionId = Guid.NewGuid().ToString("N");
        var secMsGec = GenerateSecMsGec();
        var secMsGecVersion = $"1-{CHROMIUM_FULL_VERSION}";
        
        var uri = new Uri($"{WSS_URL}?Ocp-Apim-Subscription-Key={API_KEY}&ConnectionId={connectionId}&Sec-MS-GEC={secMsGec}&Sec-MS-GEC-Version={secMsGecVersion}");
        
        try
        {
            await client.ConnectAsync(uri, CancellationToken.None);
            _logger.LogInformation("Connected to Microsoft Edge TTS WebSocket");

            // Send SSML request
            var requestId = Guid.NewGuid().ToString("N");
            var timestamp = DateTime.UtcNow.ToString("yyyy-MM-ddTHH:mm:ss.fffK");
            
            var ssml = GenerateSsml(text, voice, rate, volume, pitch);
            var configMessage = $"X-Timestamp:{timestamp}\r\nContent-Type:application/json; charset=utf-8\r\nPath:speech.config\r\n\r\n" +
                               $"{{\"context\":{{\"synthesis\":{{\"audio\":{{\"metadataoptions\":{{\"sentenceBoundaryEnabled\":\"false\",\"wordBoundaryEnabled\":\"false\"}},\"outputFormat\":\"audio-24khz-48kbitrate-mono-mp3\"}}}}}}}}";
            
            await client.SendAsync(
                new ArraySegment<byte>(Encoding.UTF8.GetBytes(configMessage)),
                WebSocketMessageType.Text,
                true,
                CancellationToken.None);

            var ssmlMessage = $"X-RequestId:{requestId}\r\nContent-Type:application/ssml+xml\r\nPath:ssml\r\n\r\n{ssml}";
            
            await client.SendAsync(
                new ArraySegment<byte>(Encoding.UTF8.GetBytes(ssmlMessage)),
                WebSocketMessageType.Text,
                true,
                CancellationToken.None);

            // Receive audio data
            var audioChunks = new List<byte>();
            var buffer = new byte[16384];
            
            while (client.State == WebSocketState.Open)
            {
                var result = await client.ReceiveAsync(new ArraySegment<byte>(buffer), CancellationToken.None);
                
                if (result.MessageType == WebSocketMessageType.Close)
                {
                    _logger.LogInformation("WebSocket closed by server");
                    break;
                }

                if (result.MessageType == WebSocketMessageType.Binary)
                {
                    _logger.LogInformation($"Received binary message: {result.Count} bytes");
                    
                    // Message must be at least 2 bytes to contain header length
                    if (result.Count < 2)
                    {
                        _logger.LogWarning("Binary message too short (< 2 bytes)");
                        continue;
                    }
                    
                    // First 2 bytes contain header length in BIG-ENDIAN format
                    var headerLength = (buffer[0] << 8) | buffer[1];
                    _logger.LogInformation($"Header length (big-endian): {headerLength}");
                    
                    // Header starts at byte 2 and continues for headerLength bytes
                    // Audio data starts after that
                    var audioStart = 2 + headerLength;
                    
                    if (audioStart > result.Count)
                    {
                        _logger.LogWarning($"Header length {headerLength} exceeds message size {result.Count}");
                        continue;
                    }
                    
                    // Parse headers to check if this is audio data
                    var headerBytes = buffer.Skip(2).Take(headerLength).ToArray();
                    var headerText = Encoding.UTF8.GetString(headerBytes);
                    
                    if (headerText.Contains("Path:audio"))
                    {
                        var audioBytes = result.Count - audioStart;
                        audioChunks.AddRange(buffer.Skip(audioStart).Take(audioBytes));
                        _logger.LogInformation($"Added {audioBytes} bytes of audio data. Total so far: {audioChunks.Count} bytes");
                    }
                    else
                    {
                        _logger.LogInformation($"Binary message is not audio data: {headerText.Substring(0, Math.Min(50, headerText.Length))}");
                    }
                }
                else if (result.MessageType == WebSocketMessageType.Text)
                {
                    var message = Encoding.UTF8.GetString(buffer, 0, result.Count);
                    _logger.LogInformation($"Received text message: {message.Substring(0, Math.Min(100, message.Length))}...");
                    
                    // Check if this is the end message
                    if (message.Contains("Path:turn.end"))
                    {
                        _logger.LogInformation("Received turn.end - audio generation complete");
                        break;
                    }
                }
            }

            await client.CloseAsync(WebSocketCloseStatus.NormalClosure, "Done", CancellationToken.None);
            
            _logger.LogInformation($"Total audio data collected: {audioChunks.Count} bytes");
            
            return audioChunks.ToArray();
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Error generating audio via WebSocket");
            return null;
        }
    }

    private string GenerateSsml(string text, string voice, string rate, string volume, string pitch)
    {
        return $@"<speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='cs-CZ'>
            <voice name='{voice}'>
                <prosody rate='{rate}' volume='{volume}' pitch='{pitch}'>
                    {System.Security.SecurityElement.Escape(text)}
                </prosody>
            </voice>
        </speak>";
    }

    private string GenerateCacheFileName(string text, string voice, string rate, string volume, string pitch)
    {
        var safeName = new string(text
            .Take(50)
            .Select(c => char.IsLetterOrDigit(c) ? c : '-')
            .ToArray())
            .Trim('-')
            .ToLowerInvariant();

        var parameters = $"{voice}{rate}{volume}{pitch}";
        var hash = Convert.ToHexString(MD5.HashData(Encoding.UTF8.GetBytes(parameters)))[..8];

        return $"{safeName}-{hash}.mp3";
    }

    private async Task PlayAudioAsync(string audioFile, string spokenText)
    {
        // Acquire speech lock
        using var lockFile = new FileStream(_speechLockFile, FileMode.OpenOrCreate, FileAccess.ReadWrite, FileShare.None);
        
        try
        {
            // Mark assistant as speaking BEFORE playback
            await _assistantSpeechState.StartSpeakingAsync();
            
            // Notify ContinuousListener what we're about to say
            await NotifyListenerSpeechStartAsync(spokenText);
            
            var process = new Process
            {
                StartInfo = new ProcessStartInfo
                {
                    FileName = "ffplay",
                    Arguments = $"-nodisp -autoexit -loglevel quiet \"{audioFile}\"",
                    RedirectStandardOutput = true,
                    RedirectStandardError = true,
                    UseShellExecute = false
                }
            };

            // Store reference for stop functionality
            lock (_processLock)
            {
                _currentPlaybackProcess = process;
            }

            process.Start();
            
            // Poll every 100ms during playback to check if we should stop
            while (!process.HasExited)
            {
                // Check if CapsLock is pressed (user wants to speak)
                if (IsCapsLockOn())
                {
                    _logger.LogInformation("CapsLock detected during playback - stopping TTS immediately");
                    try
                    {
                        process.Kill(entireProcessTree: true);
                    }
                    catch (Exception ex)
                    {
                        _logger.LogDebug(ex, "Error killing ffplay process");
                    }
                    break;
                }
                
                // Wait 100ms before next check
                await Task.Delay(100);
            }
            
            // If process is still running, wait for it to finish
            if (!process.HasExited)
            {
                await process.WaitForExitAsync();
            }
        }
        finally
        {
            // Notify ContinuousListener that we stopped speaking
            await NotifyListenerSpeechEndAsync();
            
            // Mark assistant as NOT speaking AFTER playback
            await _assistantSpeechState.StopSpeakingAsync();
            
            lock (_processLock)
            {
                _currentPlaybackProcess = null;
            }
            
            lockFile.Close();
            File.Delete(_speechLockFile);
        }
    }
    
    /// <summary>
    /// Notifies ContinuousListener that assistant is starting to speak.
    /// </summary>
    private async Task NotifyListenerSpeechStartAsync(string text)
    {
        try
        {
            var response = await _httpClient.PostAsJsonAsync(
                $"{_listenerApiUrl}/api/assistant-speech/start", 
                new { text });
            
            if (!response.IsSuccessStatusCode)
            {
                _logger.LogWarning("Failed to notify listener of speech start: {Status}", response.StatusCode);
            }
        }
        catch (Exception ex)
        {
            // Don't fail TTS if listener notification fails
            _logger.LogDebug(ex, "Could not notify listener of speech start (listener may not be running)");
        }
    }
    
    /// <summary>
    /// Notifies ContinuousListener that assistant stopped speaking.
    /// </summary>
    private async Task NotifyListenerSpeechEndAsync()
    {
        try
        {
            var response = await _httpClient.PostAsync(
                $"{_listenerApiUrl}/api/assistant-speech/end", null);
            
            if (!response.IsSuccessStatusCode)
            {
                _logger.LogWarning("Failed to notify listener of speech end: {Status}", response.StatusCode);
            }
        }
        catch (Exception ex)
        {
            // Don't fail TTS if listener notification fails
            _logger.LogDebug(ex, "Could not notify listener of speech end (listener may not be running)");
        }
    }

    /// <summary>
    /// Stops current speech playback immediately.
    /// </summary>
    /// <returns>True if playback was stopped, false if nothing was playing.</returns>
    public bool StopSpeaking()
    {
        lock (_processLock)
        {
            if (_currentPlaybackProcess == null || _currentPlaybackProcess.HasExited)
            {
                _logger.LogInformation("StopSpeaking: No active playback to stop");
                return false;
            }

            try
            {
                _logger.LogInformation("StopSpeaking: Killing ffplay process {Pid}", _currentPlaybackProcess.Id);
                _currentPlaybackProcess.Kill(entireProcessTree: true);
                return true;
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "StopSpeaking: Failed to kill process");
                return false;
            }
        }
    }

    private static string ExpandPath(string path)
    {
        if (path.StartsWith("~/"))
        {
            return Path.Combine(Environment.GetFolderPath(Environment.SpecialFolder.UserProfile), path[2..]);
        }
        return path;
    }

    private static string GenerateSecMsGec()
    {
        // Get current Unix timestamp
        var ticks = DateTimeOffset.UtcNow.ToUnixTimeSeconds();
        
        // Switch to Windows file time epoch (1601-01-01 00:00:00 UTC)
        ticks += WIN_EPOCH;
        
        // Round down to nearest 5 minutes (300 seconds)
        ticks -= ticks % 300;
        
        // Convert to 100-nanosecond intervals (Windows file time format)
        var ticksInNs = (double)ticks * S_TO_NS / 100;
        
        // Create string to hash
        var strToHash = $"{ticksInNs:F0}{API_KEY}";
        
        // Compute SHA256 hash and return uppercased hex digest
        var hashBytes = SHA256.HashData(Encoding.ASCII.GetBytes(strToHash));
        return Convert.ToHexString(hashBytes);
    }

    public int ClearCache()
    {
        try
        {
            var files = Directory.GetFiles(_cacheDirectory, "*.mp3");
            foreach (var file in files)
            {
                File.Delete(file);
            }
            return files.Length;
        }
        catch
        {
            return 0;
        }
    }
}
